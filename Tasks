/* Need to change de data load due an quota limit on apex enviroment */

--- in average this will take 1s to be executed
insert into item(item,dept,item_desc)
select level, round(DBMS_RANDOM.value(1,100)), translate(dbms_random.string('a', 20), 'abcXYZ', level) from dual connect by level <= 500 --10000;

--- in average this will take 1s to be executed
insert into loc(loc,loc_desc)
select level+100, translate(dbms_random.string('a', 20), 'abcXYZ', level) from dual connect by level <= 50 --1000;

-- in average this will take less than 120s to be executed
insert into item_loc_soh (item, loc, dept, unit_cost, stock_on_hand)
select item, loc, dept, (DBMS_RANDOM.value(5000,50000)), round(DBMS_RANDOM.value(1000,100000))
from item, loc;

commit;

/************************** ITEM 1 ****************************/
-- considering index on table item
create index item_idx on item(item);

-- considering index on table loc
create index loc_idx on loc(loc);

-- considering index on table item_loc_soh
create index item_loc_idx on item_loc_soh(item);

-- if item is unique -- adding PK on item table
ALTER TABLE item
ADD CONSTRAINT item_pk PRIMARY KEY (item);

-- if loc is unique -- adding PK on loc table
ALTER TABLE loc
ADD CONSTRAINT loc_pk PRIMARY KEY (loc);

-- also add fk in item and loc columns on table item_loc _soh
ALTER TABLE item_loc_soh
ADD CONSTRAINT fk_item
  FOREIGN KEY (item)
  REFERENCES item(item);

ALTER TABLE item_loc_soh
ADD CONSTRAINT fk_loc
  FOREIGN KEY (loc)
  REFERENCES loc(loc);

/************************** ITEM 2 ****************************

Some ideas :
    1 - Indexing: Proper indexing can significantly speed up data retrieval operations, especially for large tables. Analyze query patterns and create indexes on columns frequently used in WHERE clauses or JOIN conditions.
    2 - Partitioning: Partitioning divides large tables into smaller, more manageable parts, which can improve query performance by allowing the database to access only relevant partitions. Consider partitioning based on date ranges, regions, or other logical divisions.
    3 - Materialized Views: Materialized views store the results of a query as a physical table, allowing for faster access and reduced query processing time. They are particularly useful for complex queries or aggregations on large datasets.
    4 - Query Optimization: Review and optimize SQL queries to ensure they are written efficiently. Use EXPLAIN PLAN to analyze query execution paths and identify potential bottlenecks. Consider rewriting queries to use more efficient joins, filters, and indexes.
    5 - Bulk Processing: Utilize bulk processing techniques like bulk inserts, updates, and deletes to minimize transaction overhead and improve performance when dealing with large datasets.
    6 - Compression: Oracle offers various compression techniques to reduce storage space and improve I/O performance for large tables. Evaluate options such as Advanced Row Compression or Advanced Compression to optimize storage efficiency.
    7 - Database Statistics: Keep database statistics up-to-date to help the query optimizer make informed decisions about query execution plans. Regularly gather statistics on tables, indexes, and partitions using DBMS_STATS package or automatic statistics collection.
    8 - Tuning Parameters: Adjust database configuration parameters such as memory allocation, I/O settings, and parallel processing parameters to optimize performance for large data operations.
    9 - Use of Index-Organized Tables (IOT): For tables with primary key or unique key constraints and a small number of columns frequently queried together, consider using IOTs to improve access speed by storing the entire row within the index structure.
    10 - Data Archiving and Purging: Implement data archiving and purging strategies to remove obsolete or historical data from large tables, reducing the overall dataset size and improving query performance.
    11 - Caching: Utilize Oracle's caching mechanisms such as result cache, buffer cache, or SQL query result cache to store frequently accessed data in memory, reducing disk I/O overhead and improving response times.

*/
/********************* ITEM 3 ***********************************

    1 - Optimistic Locking: Use optimistic locking techniques such as version numbers or timestamps to manage concurrent access to rows. This approach allows multiple transactions to read and modify data simultaneously, only detecting conflicts during the update phase.
    2 - Partitioning: Partition large tables to distribute data across multiple physical segments. This can reduce contention by allowing concurrent transactions to operate on different partitions simultaneously, minimizing contention on individual rows.
    3 - Indexing: Ensure that appropriate indexes are in place to support queries and enforce uniqueness constraints without causing excessive contention. However, be cautious with indexes on frequently updated columns, as they can increase contention due to index maintenance overhead.
*/

/************************** ITEM 4 ****************************/

CREATE VIEW v_item_loc_soh (item, loc, dept)
	AS SELECT item, loc, dept 
    from item, loc ;

/************************** ITEM 5 ****************************/
 create table dept(
    dept number(4) not null,
    dept_desc varchar2(25) not null,
    user_id number(10) not null
);


create index dept_idx on dept(dept);
-- or --
ALTER TABLE dept
ADD CONSTRAINT dept_pk PRIMARY KEY (dept);




/************************** ITEM 6 ****************************/

create table item_loc_soh_plus (item varchar2(25) not null,
loc number(10) not null,
dept number(4) not null,
unit_cost number(20,4) not null,
stock_on_hand number(12,4) not null,
stock_value number(20,4) not null)

CREATE OR REPLACE PACKAGE store_management_pkg AS
    -- Procedure to save item_loc_soh data to a new table with stock value
    PROCEDURE save_item_loc_soh(
        p_store_id IN NUMBER
    );
END store_management_pkg;
/


CREATE OR REPLACE PACKAGE BODY store_management_pkg AS
    -- Procedure to save item_loc_soh data to a new table with stock value
    PROCEDURE save_item_loc_soh(
        p_store_id IN NUMBER
    ) AS
    BEGIN
        -- Insert data into the new table with calculated stock value
        INSERT INTO item_loc_soh_plus (item, loc, stock_on_hand, unit_cost, stock_value)
        SELECT item, loc, stock_on_hand, unit_cost, unit_cost * stock_on_hand AS stock_value
        FROM item_loc_soh
        WHERE loc = p_store_id;
        COMMIT;
    END save_item_loc_soh;
END store_management_pkg;
/


/************************** ITEM 7 ****************************/


CREATE OR REPLACE PROCEDURE filter_data_by_dept(
    p_user_id IN NUMBER
) AS
    v_sql_query VARCHAR2(4000);
BEGIN
    -- Construct SQL query based on user's department association
    v_sql_query := 'SELECT * FROM dept WHERE ';
    
    -- Add department filter based on user ID
    IF p_user_id = 1 THEN -- Example condition, replace with your logic
        v_sql_query := v_sql_query || 'user_id = 1';
    ELSIF p_user_id = 2 THEN
        v_sql_query := v_sql_query || 'user_id = 2';
    ELSE
        -- Default filter for users not associated with any department
        v_sql_query := v_sql_query || '1 = 0'; -- No rows returned
    END IF;

    -- Execute the dynamic SQL query
    EXECUTE IMMEDIATE v_sql_query;
END filter_data_by_dept;
/

/************************** ITEM 8 ****************************/

-- Create a nested table type
CREATE OR REPLACE TYPE location_list_type AS TABLE OF VARCHAR2(100);

-- Create the pipeline function
CREATE OR REPLACE FUNCTION get_location_list RETURN location_list_type PIPELINED IS
BEGIN
    -- You can replace this query with any logic to fetch locations
    FOR location_rec IN (SELECT dept FROM dept) LOOP
        PIPE ROW(location_rec.location_name);
    END LOOP;
    RETURN;
END;
/

/************************** ITEM 9 ****************************

In this plan we have a full access in table with more than 10k lines!

------------------------------------------------------------------------------
| Id  | Operation           | Name         | Rows | Bytes | Cost  | Time       |
------------------------------------------------------------------------------
|   0 | SELECT STATEMENT    |              | 100019 | 40760 | 10840 | 00:00:03 |
| * 1 |   TABLE ACCESS FULL | ITEM_LOC_SOH | 100019 | 40760 | 10840 | 00:00:03 |
------------------------------------------------------------------------------

To optimize this query we put an index on some fields to optmize and reduce the cost and running statistics!

Plan hash value: 1697218418

----------------------------------------------------------------------------------
| Id  | Operation         | Name         | Rows  | Bytes | Cost (%CPU)| Time     |
----------------------------------------------------------------------------------
|   0 | SELECT STATEMENT  |              |    11M|   702M| 11116   (2)| 00:00:01 |
|   1 |  TABLE ACCESS FULL| ITEM_LOC_SOH |    11M|   702M| 11116   (2)| 00:00:01 |
----------------------------------------------------------------------------------

Plan hash value: 1633205274

-------------------------------------------------------------------------------------
| Id  | Operation            | Name         | Rows  | Bytes | Cost (%CPU)| Time     |
-------------------------------------------------------------------------------------
|   0 | SELECT STATEMENT     |              |    11M|   149M|  6152   (2)| 00:00:01 |
|   1 |  INDEX FAST FULL SCAN| ITEM_LOC_IDX |    11M|   149M|  6152   (2)| 00:00:01 |
-------------------------------------------------------------------------------------

*/
/************************** ITEM 10 ****************************

cannot make tests due a quota limit on apex enviroment
maybe using bulk collect(?)


DECLARE
    v_start_time TIMESTAMP;
BEGIN
    -- Record start time
    v_start_time := SYSTIMESTAMP;

    1 -- ALTER TABLE your_table_name DISABLE ALL CONSTRAINTS;

    2 -- Perform the migration
    
    3 -- ALTER TABLE your_table_name ENABLE ALL CONSTRAINTS;

    4 -- COMMIT;

    -- Calculate execution time
    DBMS_OUTPUT.PUT_LINE('Migration completed in: ' || (SYSTIMESTAMP - v_start_time));
END;
/
*/

/************************** ITEM 11 ****************************

AWR report indicates huge wait events
maybe changing the commit_logging parameter increase de performance.

BATCH/IMMEDIATE

SQL> show parameter commit

NAME                                 TYPE        VALUE
———————————— ———– ——————————
commit_logging                    string
commit_point_strength         integer     1
commit_wait                          string
commit_write                         string

ALTER SYSTEM SET COMMIT_LOGGING=BATCH SCOPE=BOTH;

shwo show parameter commit

NAME                                 TYPE        VALUE
———————————— ———– ——————————
commit_logging                    string       BATCH
commit_point_strength         integer     1
commit_wait                          string
commit_write                         string
*/

/************************** ITEM 12 ****************************/
/* Note : Use Python language                                  */

import csv
import cx_Oracle

def extract_to_csv(connection_string, table_name):
    try:
        # Connect to the Oracle database
        connection = cx_Oracle.connect(connection_string)
        cursor = connection.cursor()

        # Execute the query to select distinct loc from the item_loc_soh_plus
        cursor.execute(f"SELECT DISTINCT location FROM {item_loc_soh_plus")

        # Fetch all distinct locations from the result set
        locations = cursor.fetchall()

        # Iterate over each location
        for location in locations:
            location = location[0]  # Extract the location value from the tuple
            
            # Execute the query to select data for the current location
            cursor.execute(f"SELECT item, loc, dept, unit_cost, stock_on_hand, stock_value FROM {item_loc_soh_plus} WHERE location = :loc", loc=location)

            # Fetch all rows for the current location
            rows = cursor.fetchall()

            # Write the data to a CSV file for the current location
            with open(f"{location}.csv", 'w', newline='') as csvfile:
                csv_writer = csv.writer(csvfile)
                # Write the header
                csv_writer.writerow(['Item', 'Location', 'Dept', 'Unit Cost', 'Hand Quantity', 'Stock Value'])
                # Write the rows
                csv_writer.writerows(rows)

            print(f"Data extracted for location '{location}' to '{location}.csv' successfully.")

    except cx_Oracle.Error as error:
        print(f"Error occurred: {error}")
    finally:
        if cursor:
            cursor.close()
        if connection:
            connection.close()

# call
if __name__ == "__main__":
    # Provide connection string
    connection_string = "username/password@host:port/service_name"
    # Provide table name
    table_name = "your_table_name"
    
    # Call the function to extract data to CSV for each location
    extract_to_csv(connection_string, table_name)


